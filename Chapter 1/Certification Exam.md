### Certification Exam MCQ Answers
Question 1
What is the primary advantage of using Transfer Learning when working with Transformer models?

--> It enables leveraging knowledge from a pretrained model for a new task, reducing data, time, and resource requirements for fine-tuning.


Question 2
Which pretraining objective involves masking tokens in the input and training the model to predict the original tokens, commonly associated with encoder models like BERT?

---> Masked Language Modeling (MLM)


Question 3
In Large Language Model inference, what does the 'temperature' sampling parameter primarily control?

---> The randomness/creativity of the output (lower is more focused/deterministic, higher is more random).


Question 4
Which of the following is considered a significant limitation of Large Language Models (LLMs)?

---> Potential to generate incorrect or nonsensical information confidently (hallucinations).


Question 5
What are the two distinct phases typically involved in the Large Language Model (LLM) inference process for generating text after receiving a prompt?

---> Prefill (processing the prompt) and Decode (generating tokens sequentially)


Question 6
What are the three main high-level steps a Hugging Face pipeline() typically performs when processing input text for inference?


---> Preprocessing, Model Inference, Postprocessing


Question 7
Which type of Transformer architecture is generally best suited for sequence-to-sequence tasks like translation or summarization?

---> Encoder-decoder (e.g., T5, BART)


Question 8
What is the main purpose of the Key-Value (KV) Cache optimization used during LLM inference?

---> To store and reuse intermediate attention calculations from previous steps, speeding up the token generation (decode phase).


Question 9
What is a key difference between the broader field of Natural Language Processing (NLP) and Large Language Models (LLMs)?

---> NLP is a broader field, while LLMs are a powerful subset known for their size and general capabilities.


Question 10
What are the three main architectural categories commonly used for Transformer models?

---> Encoder-only, Decoder-only, Encoder-decoder (Sequence-to-sequence)
